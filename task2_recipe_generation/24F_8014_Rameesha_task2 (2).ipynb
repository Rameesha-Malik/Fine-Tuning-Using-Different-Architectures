{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rameesha - MSDS - 24F-8014"
      ],
      "metadata": {
        "id": "wudor49l7XBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Decoder Model (GPT-2) — Recipe Generation**"
      ],
      "metadata": {
        "id": "YjDetUSn7USb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:**\n",
        "Generate\tcooking\trecipes\tgiven\ta\tlist\tof\tingredients\tor\ta\trecipe\ttitle.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "https://www.kaggle.com/datasets/nazmussakibrupol/3a2mext/data\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Objective:**\n",
        "Fine-tune\tGPT-2\tto\tgenerate\tcoherent\tand\tcreative\trecipes.\tInput\tmay\tbe\ta\tlist\tof ingredients\tor\ta\tdish\tname,\tand\tthe\toutput\tshould\tbe\ta\trecipe\twith\tsteps.\n",
        "\n",
        "---\n",
        "\n",
        "**Deliverables:**\n",
        "* Tokenization\tand\tdataset\tformatting\tscript\n",
        "* Training\tloop\tfor\tGPT-2\n",
        "* Example\tgenerations\tand\tquality\tevaluation\t(e.g.,\tROUGE,\tBLEU,\thuman evaluation)\n",
        "* Streamlit/Gradio\tapp\tfor\tinteractive\trecipe\tgeneration"
      ],
      "metadata": {
        "id": "FsWhwI0S7b1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "N8PHCQDxAWuA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIHIHjSJAeNE",
        "outputId": "51e88e44-96c4-4f44-84bd-4ec5c9fab9df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dataset**"
      ],
      "metadata": {
        "id": "LUvfd3c-BLCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/ANLP/project_2/task_2/'\n",
        "files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
        "df = pd.read_csv(os.path.join(path, files[0]))\n",
        "\n",
        "print(f\"loaded dataset with {len(df)} recipes\")\n",
        "print(f\"columns found: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaDNfXyOBLhL",
        "outputId": "7c8eb4a4-b355-47ac-b49b-e15ea2dcaa3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded dataset with 2231143 recipes\n",
            "columns found: ['title', 'NER', 'Extended_NER', 'genre', 'label', 'directions']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data clean and prep**"
      ],
      "metadata": {
        "id": "IWwQkvbVBeWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find correct column names automatically\n",
        "cols = df.columns\n",
        "title_col = [c for c in cols if 'title' in c.lower()][0]\n",
        "ingr_col = [c for c in cols if 'ingredient' in c.lower() or 'ner' in c.lower()][0]\n",
        "dir_col = [c for c in cols if 'direction' in c.lower() or 'step' in c.lower()][0]\n",
        "\n",
        "# del missing rows\n",
        "df_clean = df[[title_col, ingr_col, dir_col]].dropna()\n",
        "\n",
        "# take smaller sample for faster training\n",
        "df_sample = df_clean.sample(n=min(3000, len(df_clean)), random_state=42)\n",
        "\n",
        "# combine title ing  and directions in 1 text\n",
        "df_sample['text'] = df_sample.apply(\n",
        "    lambda x: f\"Title: {x[title_col]} | Ingredients: {x[ingr_col]} | Directions: {x[dir_col]}<|endoftext|>\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# split into train  val sets\n",
        "train_texts, val_texts = train_test_split(df_sample['text'].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"train samples: {len(train_texts)}, validation samples: {len(val_texts)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHqMT_TCBZSk",
        "outputId": "62f91f8c-98de-4bc4-cb50-5cf2b9be99e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train samples: 2400, validation samples: 600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load GPT 2**"
      ],
      "metadata": {
        "id": "zmkQI6VwB2gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"model ready on {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cef8-YkqB3C3",
        "outputId": "54fec9ce-ca57-4294-bd85-53c5b9313faa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model ready on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**daatset nd dataloadder**"
      ],
      "metadata": {
        "id": "dGhKKsztDJiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom dataset class\n",
        "class RecipeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=256):\n",
        "        self.encodings = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()\n",
        "        return item\n",
        "\n",
        "# create datasets and loaders\n",
        "train_dataset = RecipeDataset(train_texts, tokenizer)\n",
        "val_dataset = RecipeDataset(val_texts, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "print(\"dataset dataloader creation done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGycrvcGDJ-e",
        "outputId": "847baf5e-34f6-4de5-9405-10da4ff65d3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset dataloader creation done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**model training**"
      ],
      "metadata": {
        "id": "KrUT2XB0EEaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nepoch {epoch+1}/{epochs} started\")\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # training loop\n",
        "    for batch in tqdm(train_loader, desc=\"training\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"average validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\ntraining done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN1k48vPD9MJ",
        "outputId": "6c7fa3f3-fe59-4906-9a06-ea2ca85b1104"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 1/2 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 600/600 [03:03<00:00,  3.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average training loss: 1.4412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation: 100%|██████████| 150/150 [00:12<00:00, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average validation loss: 1.2908\n",
            "\n",
            "epoch 2/2 started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 600/600 [03:04<00:00,  3.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average training loss: 1.2781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation: 100%|██████████| 150/150 [00:12<00:00, 11.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average validation loss: 1.2599\n",
            "\n",
            "training done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**saved finetune model**"
      ],
      "metadata": {
        "id": "PZeozUmoEQn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/ANLP/project_2/task_2/gpt2_recipe_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/ANLP/project_2/task_2/gpt2_recipe_model')\n",
        "\n",
        "print(\"model nd tokenizer saved to /content/drive/MyDrive/ANLP/project_2/task_2/gpt2_recipe_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YQ-sydhEPU5",
        "outputId": "0f71a2d5-4317-4f8e-9150-4fea30c3c654"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model nd tokenizer saved to /content/drive/MyDrive/ANLP/project_2/task_2/gpt2_recipe_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test with my inputs**"
      ],
      "metadata": {
        "id": "E5s0mV0MFQ8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_length=200):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# few test examples\n",
        "prompts = [\"Title: Chocolate Cake |\", \"Title: Chicken Soup |\"]\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nprompt: {p}\")\n",
        "    print(f\"generated recipe: {generate(p)[:200]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7VFCB2CFPhm",
        "outputId": "193c23e8-b5ed-4383-ab0e-ae9232602fa0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "prompt: Title: Chocolate Cake |\n",
            "generated recipe: Title: Chocolate Cake | Ingredients: [\"cocoa\", \"vanilla pudding\", \"eggs\", \"sugar\", \"vanilla pudding\", \"chocolate chips\"] | Directions: [\"Preheat oven to 350 degrees F (175 degrees C). Grease and flour...\n",
            "\n",
            "prompt: Title: Chicken Soup |\n",
            "generated recipe: Title: Chicken Soup | Ingredients: [\"chicken broth\", \"onion\", \"garlic\", \"onions\", \"curry powder\", \"salt\", \"pepper\", \"thyme\", \"sour cream\", \"black pepper\", \"pepper\"] | Directions: [\"Combine all ingredi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u8IPN5BXI0H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk rouge-score\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "id": "nMboxgskKAB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a few samples from validation set for quick testing\n",
        "test_samples = val_texts[:10]\n",
        "references = []\n",
        "candidates = []\n",
        "\n",
        "# generate predictions and store references\n",
        "for recipe in test_samples:\n",
        "    title_part = recipe.split('|')[0] + '|'\n",
        "    generated = generate(title_part, max_length=200)\n",
        "    references.append([recipe.split()])\n",
        "    candidates.append(generated.split())\n",
        "\n",
        "# calculate BLEU score\n",
        "bleu_score = corpus_bleu(references, candidates)\n",
        "print(f\"bleu score: {bleu_score:.4f}\")\n",
        "\n",
        "# calculate ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "\n",
        "for ref, cand in zip([r[0] for r in references], [' '.join(c) for c in candidates]):\n",
        "    ref_text = ' '.join(ref)\n",
        "    scores = scorer.score(ref_text, cand)\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] += scores[key].fmeasure\n",
        "\n",
        "# average scores\n",
        "for key in rouge_scores:\n",
        "    rouge_scores[key] /= len(test_samples)\n",
        "\n",
        "print(f\"rouge-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"rouge-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"rouge-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "print(\"\\nevaluation done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7FfRpEAI0iw",
        "outputId": "cc05226d-40a2-4414-c8f4-e5cbea566519"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bleu score: 0.0781\n",
            "rouge-1: 0.3119\n",
            "rouge-2: 0.0878\n",
            "rouge-L: 0.2201\n",
            "\n",
            "evaluation done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**save report**"
      ],
      "metadata": {
        "id": "9pJx80JyMIeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# create a summary dictionary\n",
        "results_summary = {\n",
        "    \"model_name\": \"gpt2_recipe_model\",\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"bleu_score\": round(bleu_score, 4),\n",
        "    \"rouge1\": round(rouge_scores[\"rouge1\"], 4),\n",
        "    \"rouge2\": round(rouge_scores[\"rouge2\"], 4),\n",
        "    \"rougeL\": round(rouge_scores[\"rougeL\"], 4),\n",
        "    \"total_test_samples\": len(test_samples)\n",
        "}\n",
        "\n",
        "# save path in google drive\n",
        "save_path = '/content/drive/MyDrive/ANLP/project_2/task_2/evaluation_summary.json'\n",
        "\n",
        "# write to json file\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "\n",
        "print(f\"evaluation summary saved to: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWP1kRpkMLQZ",
        "outputId": "c0d9095a-7190-4c43-813d-9ad6bd9be205"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation summary saved to: /content/drive/MyDrive/ANLP/project_2/task_2/evaluation_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**grdaio**"
      ],
      "metadata": {
        "id": "SJzgkKydQh7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# load fine-tuned model\n",
        "model_path = '/content/drive/MyDrive/ANLP/project_2/task_2/gpt2_recipe_model'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
        "model.eval()\n",
        "\n",
        "# clean and format recipe text\n",
        "def clean_recipe_text(text):\n",
        "    text = text.replace(\"<|endoftext|>\", \"\").encode(\"utf-8\").decode(\"unicode_escape\")\n",
        "\n",
        "    # extract sections\n",
        "    title_match = re.search(r\"Title:\\s*(.*?)\\s*\\|\", text)\n",
        "    ingr_match = re.search(r\"Ingredients:\\s*(.*?)\\s*\\|\", text)\n",
        "    dir_match = re.search(r\"Directions:\\s*(.*)\", text)\n",
        "\n",
        "    title = title_match.group(1).strip() if title_match else \"Untitled\"\n",
        "    ingredients = ingr_match.group(1).replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '').replace(\"'\", '')\n",
        "    directions = dir_match.group(1).replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', '').replace(\"'\", '')\n",
        "\n",
        "    # split items neatly\n",
        "    ingr_list = [i.strip() for i in re.split(r\",\\s*\", ingredients) if i.strip()]\n",
        "    dir_list = [d.strip() for d in re.split(r\"\\.\\s*\", directions) if d.strip()]\n",
        "\n",
        "    # build formatted output\n",
        "    formatted = f\"**Title:** {title}\\n\\n**Ingredients:**\\n\"\n",
        "    formatted += \"\\n\".join(f\"- {i}\" for i in ingr_list)\n",
        "    formatted += \"\\n\\n**Directions:**\\n\"\n",
        "    formatted += \"\\n\".join(f\"{idx+1}. {step}\" for idx, step in enumerate(dir_list))\n",
        "    return formatted.strip()\n",
        "\n",
        "\n",
        "# generate recipe text\n",
        "def generate_recipe(prompt):\n",
        "    if not prompt.strip():\n",
        "        return \"please enter a dish name or ingredients.\"\n",
        "\n",
        "    input_text = f\"Title: {prompt} |\"\n",
        "    inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=300,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return clean_recipe_text(raw)\n",
        "\n",
        "\n",
        "# color and style settings\n",
        "main_color = \"#FF6F61\"  # warm coral (food tone)\n",
        "accent_color = \"#FFA94D\"  # soft orange accent\n",
        "\n",
        "# gradio interface\n",
        "app = gr.Interface(\n",
        "    fn=generate_recipe,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=2,\n",
        "        label=\"Enter dish name or ingredients\",\n",
        "        placeholder=\"e.g. Chocolate Cake or Chicken, Rice, Garlic\"\n",
        "    ),\n",
        "    outputs=gr.Markdown(label=\"Generated Recipe\"),\n",
        "    title=\"🍽️ GPT-2 Recipe Generator\",\n",
        "    description=\"Type a dish name or a few ingredients to create a full recipe.\",\n",
        "    examples=[\n",
        "        [\"Spicy Chicken Curry\"],\n",
        "        [\"Chocolate Cake\"],\n",
        "        [\"Vegetable Fried Rice\"],\n",
        "        [\"Mango Smoothie\"],\n",
        "        [\"Pasta with Garlic and Cheese\"]\n",
        "    ],\n",
        "    allow_flagging=\"never\",  # flag fully removed\n",
        "    theme=gr.themes.Default(primary_hue=gr.themes.colors.orange),\n",
        "    css=f\"\"\"\n",
        "        body {{\n",
        "            background: linear-gradient(135deg, {main_color}33, {accent_color}33);\n",
        "        }}\n",
        "        .gradio-container {{\n",
        "            border: 2px solid {main_color};\n",
        "            border-radius: 12px;\n",
        "            padding: 15px;\n",
        "        }}\n",
        "        h1, h2, h3 {{\n",
        "            color: {main_color};\n",
        "        }}\n",
        "        textarea {{\n",
        "            font-size: 15px !important;\n",
        "            height: 400px !important;\n",
        "        }}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "app.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "5ww1HN2pNPdU",
        "outputId": "4c4f0e00-44f7-4f7b-a12f-f615f66e1c8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7fec1d07c0ed5ce4ed.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7fec1d07c0ed5ce4ed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}